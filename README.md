# ğŸš€ End-to-End MLOps Pipeline

> **Industry-grade ML system with data versioning, experiment tracking, automated training, and CI/CD deployment**

[![MLOps](https://img.shields.io/badge/MLOps-Production--Ready-blue)]()
[![Python](https://img.shields.io/badge/Python-3.9+-green)]()
[![License](https://img.shields.io/badge/License-MIT-yellow)]()

---

## ğŸ“‹ Table of Contents
- [Project Overview](#project-overview)
- [Architecture](#architecture)
- [Tech Stack](#tech-stack)
- [Project Structure](#project-structure)
- [Setup Instructions](#setup-instructions)
- [Usage](#usage)
- [MLOps Workflow](#mlops-workflow)

---

## ğŸ¯ Project Overview

This project demonstrates a complete MLOps pipeline that solves real production problems:

- âœ… **Data Versioning**: Track datasets like code with DVC
- âœ… **Experiment Tracking**: Log every hyperparameter, metric, and artifact with MLflow
- âœ… **Reproducibility**: Anyone can recreate exact model results
- âœ… **Automated Training**: CI/CD pipelines handle retraining
- âœ… **Model Serving**: FastAPI endpoints for real-time predictions
- âœ… **Monitoring**: Track model performance over time

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Raw Data  â”‚â”€â”€â”€â”€>â”‚ DVC Pipeline â”‚â”€â”€â”€â”€>â”‚  Processed  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Training   â”‚
                    â”‚   (MLflow)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Model Registryâ”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  FastAPI API â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Tech Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Version Control** | Git + GitHub | Code versioning & collaboration |
| **Data Versioning** | DVC | Track datasets and data pipelines |
| **Experiment Tracking** | MLflow | Log experiments, models, metrics |
| **Model Serving** | FastAPI | REST API for predictions |
| **CI/CD** | GitHub Actions | Automated testing & deployment |
| **Containerization** | Docker | Consistent environments |
| **Language** | Python 3.9+ | Core development |

---

## ğŸ“‚ Project Structure

```
End-To-End-MLOPs/
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/           # CI/CD pipeline definitions
â”‚       â””â”€â”€ train-deploy.yml
â”‚
â”œâ”€â”€ src/                     # Source code (production-ready)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_pipeline.py     # Data preprocessing logic
â”‚   â”œâ”€â”€ train.py             # Model training script
â”‚   â”œâ”€â”€ evaluate.py          # Model evaluation
â”‚   â””â”€â”€ serve.py             # FastAPI serving logic
â”‚
â”œâ”€â”€ data/                    # Data storage (managed by DVC)
â”‚   â”œâ”€â”€ raw/                 # Original, immutable data
â”‚   â””â”€â”€ processed/           # Cleaned, feature-engineered data
â”‚
â”œâ”€â”€ models/                  # Model artifacts (managed by MLflow)
â”‚   â””â”€â”€ .gitkeep
â”‚
â”œâ”€â”€ notebooks/               # Jupyter notebooks (EDA only, not production)
â”‚   â””â”€â”€ 01_exploratory_data_analysis.ipynb
â”‚
â”œâ”€â”€ tests/                   # Data validation & model tests
â”‚   â”œâ”€â”€ test_data_pipeline.py
â”‚   â””â”€â”€ test_model.py
â”‚
â”œâ”€â”€ config/                  # Configuration files
â”‚   â””â”€â”€ config.yaml
â”‚
â”œâ”€â”€ .dvcignore              # DVC ignore patterns
â”œâ”€â”€ .gitignore              # Git ignore patterns
â”œâ”€â”€ dvc.yaml                # DVC pipeline definition (created later)
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ Dockerfile              # Container definition
â””â”€â”€ README.md               # This file
```

---

## ğŸš€ Setup Instructions

### Prerequisites
- Python 3.9 or higher
- Git installed
- (Optional) Docker for containerization

### 1ï¸âƒ£ Clone the Repository
```bash
git clone <your-repo-url>
cd End-To-End-MLOPs
```

### 2ï¸âƒ£ Create Virtual Environment
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 3ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Initialize DVC
```bash
dvc init
```

### 5ï¸âƒ£ Start MLflow Tracking Server (Optional)
```bash
mlflow ui --port 5000
```
Access at: http://localhost:5000

---

## ğŸ“– Usage

### Training Pipeline
```bash
python src/train.py
```

### Model Serving
```bash
uvicorn src.serve:app --reload
```
Access API docs at: http://localhost:8000/docs

### Run Tests
```bash
pytest tests/
```

---

## ğŸ”„ MLOps Workflow

### Phase 1: Data Preparation
1. Add raw data to `data/raw/`
2. Version data with DVC: `dvc add data/raw/dataset.csv`
3. Commit DVC metadata: `git commit -am "Track dataset"`

### Phase 2: Experimentation
1. Run training script: `python src/train.py`
2. MLflow logs metrics, params, and models automatically
3. Compare experiments in MLflow UI

### Phase 3: Model Selection
1. Choose best model from MLflow registry
2. Promote to "Production" stage
3. Model artifacts are versioned automatically

### Phase 4: Deployment
1. Push code to GitHub
2. CI/CD pipeline triggers automatically
3. Tests run â†’ Model retrains â†’ API deploys

### Phase 5: Monitoring
1. Track prediction latency
2. Monitor model performance metrics
3. Trigger retraining when drift detected

---

## ğŸ“ Learning Outcomes

By studying this project, you'll understand:

- âœ… How to version datasets efficiently
- âœ… How to track experiments systematically
- âœ… How to build reproducible ML pipelines
- âœ… How to serve models as production APIs
- âœ… How to automate ML workflows with CI/CD
- âœ… How to structure ML projects for collaboration

---

## ğŸ“ Interview Talking Points

**Q: How do you ensure model reproducibility?**  
A: We use the "versioning trinity": Git for code, DVC for data, and MLflow for models. Every training run logs the exact code version, data version, and hyperparameters used.

**Q: How do you handle model deployment?**  
A: We use FastAPI to serve models as REST APIs. GitHub Actions automates testing and deployment. Every model version is containerized with Docker for consistency.

**Q: What happens when data drifts?**  
A: We monitor prediction distributions and model performance. When drift is detected, we trigger an automated retraining pipeline through our CI/CD system.

---

## ğŸ¤ Contributing

This is a learning project. Feel free to:
- Add new features
- Improve documentation
- Report issues
- Suggest better practices

---

## ğŸ“„ License

MIT License - See LICENSE file for details

---

## ğŸ”— Resources

- [MLflow Documentation](https://mlflow.org/docs/latest/index.html)
- [DVC Documentation](https://dvc.org/doc)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)

---

**Built with â¤ï¸ for production ML systems**
